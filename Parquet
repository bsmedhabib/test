import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;

import org.apache.hadoop.conf.Configuration;
import org.apache.parquet.column.ColumnDescriptor;
import org.apache.parquet.column.impl.ColumnReaderImpl;
import org.apache.parquet.hadoop.metadata.ParquetMetadata;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.io.ColumnIOFactory;
import org.apache.parquet.io.MessageColumnIO;
import org.apache.parquet.io.RecordReader;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.hadoop.api.ReadSupport;

public class ParquetReader {
  
  public void loadFromFile(String filePath) throws IOException {
    Path parquetFilePath = Paths.get(filePath);
    Configuration configuration = new Configuration();
    try (ParquetReader<Void> reader = ParquetReader.builder(new ReadSupport<Void>() {
        public ReadSupport.ReadContext init(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema) {
          return new ReadSupport.ReadContext(fileSchema);
        }

        public RecordMaterializer<Void> prepareForRead(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema, ReadSupport.ReadContext readContext) {
          return new RecordMaterializer<Void>() {
            public Void getCurrentRecord() {
              return null;
            }

            public Void getCurrentRecord(Void reuse) {
              return null;
            }
          };
        }
      }, parquetFilePath)
        .withConf(configuration)
        .build()) {
      MessageType schema = reader.getFooter().getFileMetaData().getSchema();
      MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);
      RecordReader<Void> recordReader = columnIO.getRecordReader(reader, new ColumnReaderImpl.Builder(configuration));
      
      while (recordReader.hasNext()) {
        recordReader.read();
        extractEntry();
      }
    }
  }
  
}
